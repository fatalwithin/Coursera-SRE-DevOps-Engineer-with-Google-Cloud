In this course, each module aims to build on your ability to interact with GKE, and includes hands-on labs for you to experience functionalities first-hand. In the first module, you'll be introduced to a range of Google Cloud services and features, with a view to helping you choose the right Google Cloud services to create your own cloud solution. You'll learn about creating a container using Cloud Build, and store a container in Container Registry. You'll also compare and contrast the features of Kubernetes and Google Kubernetes Engine, also referred to as GKE. In addition to conceptualizing the Kubernetes architecture, you'll deploy a Kubernetes cluster using GKE, deploy Pods to a GKE cluster, and view and manage Kubernetes objects.

# Introduction


## Cloud Computing and Google Cloud

Let's start by introducing the relationship between Cloud computing and GCP. Cloud computing has five fundamental attributes. First, computing resources or on-demand and self-service. Cloud computing customers using automated interface and get the processing power, storage, and network they need with no human intervention. Second, resources are accessible over a network from any location. Providers allocate resources to customers from a large pool, allowing them to benefit from economies of scale. Customers don't have to know or care about the exact physical location of these resources. Resources themselves are elastic. Customers who need more resources can get them rapidly, and when they need less they can scale back. Finally, customers pay for only what they use or reserve as they go. If they stop using resources, they simply stop paying. That's how Public Clouds, such as Google Cloud Platform work. GCP offers a variety of services and architects and developers like you can choose among those services to build your solutions. Many of these Cloud services are very familiar, such as Virtual machines, on-demand. Other services represented an entirely new paradigm. One of those services is Google Kubernetes Engine, and it's the focus of this course and the ones that follow, by the way, we informally call the service GKE, and you can too. The first thing that many people ask of GCP is please run some my code in the Cloud. GCP provides a range of services for doing exactly that, each aimed at satisfying a different set of user preferences. Here's a quick summary of those choices, in a later module, we'll compare each of them in greater detail. The service that may be most familiar to newcomers is Compute Engine, which lets you run virtual machines on demand in the Cloud. It's Google Cloud's Infrastructure as a service solution, it provides maximum flexibility for people who prefer to manage those server instances themselves. Now GKE is different, it lets you run containerized applications on a Cloud environment that Google Cloud manages for you under your administrative control. What's a containerized application? And what's Kubernetes? Will learn a lot about each of those topics later on in this course. For now, you can think of containerization as a way to package code that's designed to be highly portable and to use resources very efficiently, and you can think of Kubernetes as a way to orchestrate code in those containers. App Engine is GCP is fully managed platform as a service framework. That means it's a way to run code in the Cloud without having to worry about infrastructure. You can focus on just your code and let Google deal with all the provisioning and resource management. You'll learn a lot more about App Engine in the specialization, developing applications in Google Cloud Platform. Cloud functions is a completely server less execution environment or functions as a service. It executes your code in response to events, whether those occur once a day or many times per second. Google scales resources as required, but you only pay for the service while your code runs. The specialization developing application that Google Cloud Platform also discusses Cloud Functions. In this specialization, GKE is going to be our main focus, and GKE is built on top of Compute Engine, so we'll learn more about that service too along the way. Most applications needed database of some kind. If you've built a Cloud application, you can install and run your own database forward on a virtual machine, inside of Compute Engine. You simply start up the virtual machine, install your database engine, set it up just like you would in a data center, or you can run your own database server and Google Kubernetes too however, with either approach, you have to manage and support the database yourself. Alternatively, you can use Google's fully managed database and storage services. All of these have in common is that they reduce the work it takes to store all kinds of data. GCP offers relational and non-relational databases as well as worldwide object storage. You'll learn more about these later on in this specialization. GCP also offers fully managed and big data machine learning services. Just as with storage and database services, you could build an implement these services yourself and some GCP customers do. But the fact that they're available as a service means that you can get started faster and deal with a less routine work along the way.

## Resource Management

In this video, you'll learn about the topology of GCP services and how they're distributed geographically in zones, regions and at a global scale. You'll also learn how you can structure your resources, you use hierarchically, using organizations, folders and projects to manage your access control and your billing. Behind the services provided by Google Cloud Platform lay a huge range of GCP resources, physical assets such as physical service in hard drives and virtual resources such as virtual machines in containers. These resources are managed by Google, within its global data centers, Google Cloud provides resources in multi regions, regions and zones. 

It divides the world up into three multi regional areas, the Americas, Europe and Asia Pacific. Next, the three multi regional areas, are divided into regions which are independent geographic areas on the same continent. 

Within a **region**, there's fast network connectivity, generally round trip network latencies of under a millisecond that's at the 95th percentile. 

Finally, regions are divided into **zones**, which are deployment areas for GCP resources, within a focus geographic area, you can think of a zone as like a data center within a region. Although Strictly speaking, a zone isn't necessarily a single data center. Now, we mentioned Compute engine earlier, compute engine virtual machine instances, reside within a specific zone. If that zone became unavailable, so would your virtual machines and the workloads running in them. And GKE uses compute engine, so your GKE workloads could be affected as well. Deploying applications across multiple zones enables false tolerance and high availability. In this special session will learn how to do exactly that. 

Google's data centers around the world are interconnected by the Google network, which by some publicly available estimates carries as much as 40% of the world's Internet traffic every day. This is the largest network of its kind on earth, and it continues to grow, its designed to provide the highest possible throughput, and the lowest possible latencies, for applications including yours. The network interconnects with the public Internet at more than 90 Internet exchanges and more than 100 points of presence worldwide. When an Internet user sends traffic to a Google resource, Google responds to the user's request from an edge network location that will provide the lowest latency or delay. Google's edge casting network places the content close to users to minimize that latency. 

Your applications in GCP, including those running in GKE, can take advantage of this edge network too. When you take advantage of GCP services and resources, you get to specify those resource's geographical locations. In many cases, you can also specify whether or not you're doing something at zonal level, a regional level or a multi regional level. Zonal resources operate within a single zone, which means that if the zone becomes unavailable, the resources won't be available either. A simple example, would be a compute engine virtual machine instance, and it's persistent disks. GKE has a component called a node, and these are zonal, too. Regional resources operate across multiple zones, but within one region, an application using these resources can be redundantly deployed to improve its availability. Later in this special session, you'll learn how to use GKE, so there's resources spread across different zones within a region. Cloud Data Store is an example of another GCP service that can be deployed in a similar, redundant way.

Finally, global resources can be managed across multiple regions, these resources can further improve the availability of an application. Some example of such resources include https load balancers and VPC, or virtual private cloud networks, which GKE users benefit from too. The GCP resources you use, no matter where they reside, must belong to a project. 

So what's a project? A **project** is the base level organizing entity, for creating and using resources and services and managing billing, APIs and permissions. Zones and regions physically organize the GCP resources you use, and projects logically organize them. Projects can be easily created, managed, deleted or even recovered from accidental deletions. Each project is identified by a unique project ID and project number, you can name your project and apply labels for filtering. These labels are changeable, but that project ID and project number, remain fixed. 

Projects can belong to a **folder**, which is another grouping mechanism. You should use folders to reflect their hierarchy of your enterprise and apply policies at the right levels within your enterprise. And yes, I know what you're asking, you can nest folders inside of folders if you want. For example, you can have a folder for each department with each department's folder, you can have sub folders for each of the teams that make it up. Each team's projects belong to its folder. A single organization owns all the folders beneath it. An organization is the root node of a GCP resource hierarchy. Although you're not required to have an organization to use GCP, organizations are very useful. Organizations let you set policies that apply throughout your entire enterprise. Also, having organization is required for you to use folders. If you're already a G suite customer, you have an organization already, and if not, you can get one for free through Google Cloud Identity. Your organization, while the fixed organization ID, and a changeable display name. The GCP resource hierarchy, helps you manage resources across multiple departments, and multiple teams within an organization, you can define a hierarchy that can create trust boundaries and resource isolation. For example, should members of your human resources team be able to delete running database servers, and should your engineers be able to delete the database of employees salaries? Probably not in either case, Cloud identity and access management. Also Cloud called I Am lets you fine tune access controls to all the GCP resources you use, you define I am policies that control user access to resources. You apply these policies at the level you choose, and those policies inherit downwards, for example, an I Am policy applied at the organizational level will be inherited by a folder, the project and even the resources beneath it. Additional policies at lower levels of the hierarchy can grant additional permissions. Billing, on the other hand, accumulates at the project level. Most GCP customers have a resource hierarchy that looks like their employee organization chart. While their project billing looks like their cost center structure, the resource hierarchy matters because of GCP's shared security model. When you build an application on your on premises infrastructure, you're responsible for the entire stack security. From the physical security of the hardware, and the premises in which they're housed, through the encryption of the data on disk, the integrity of your network, all the way up to securing the contents stored in those applications. But when you move an application of GCP, Google handles many of those lower levels of security, like the physical security of the hardware and its premises, the encryption of data on disk and the integrity of the physical network. Because of its scale, Google can deliver a higher level of security at these layers than most customers could afford to on their own. The upper layers of the security stack, including the securing of the data, remain your responsibility. Google does provide tools to help you implement the policies you can define at those layers. The resource hierarchy we just explored, is one of those tools, Cloud I Am is another. You'll learn more about that security in depth later on in this special session.

## Billing

Billing it's no fun. It's a fact of life. Let's learn more about it. Billing in GCP is set up at the GCP project level. When you define a GCP project, you link a billing account to it. This billing account is where you'll configure all of your billing information, including your payment option. 

You can link your billing account to one or more **projects**. Projects that you don't link to any billing account can only use free GCP services. Your billing account can be charged automatically and invoice every month or at every threshold limit. You can separate project billings by setting up billing subaccounts. Some GCP customers who resell GCP services, use those sub accounts for each of their own clients. And you're probably thinking, how can I make sure I don't accidentally run up a big GCP bill? GCP provides three tools to help, budgets and alerts, taking a look at the building export and also looking at your reports. You can define budgets at the billing account level or even at the project level. To be notified when costs approach your budget limit, you can create an alert. 

For example, with a budget limit of \$20,000 and an alert say at 90%, you'll receive a notification alert when your expenses reach that threshold, or \$18,000,. You can also set up a webbook to be called in response to an alert. This webbook can control automation based on billing alerts. For example, you can trigger the script to shut down resources when a billing alert occurs, or you can use this webbook to file a trouble ticket with your team. The billing export allows you to store detailed billing information in places where it's easy to retrieve for external analysis, such as a BigQuery data set or a cloud storage bucket. And reports is a visual tool in the console that allows you to monitor expenditure based on your projects and your services. GCP also implements quotas which limit unforeseen extra billing charges. 

Quotas are designed to prevent the overconsumption of resources because of an error or a malicious attack. Quotas apply at the level of the GCP project. 

Now there are two types of quotas, rate quotas and allocation quotas. 

**Rate quota** is reset after a specific time. For example, by default the GKE service implements a quote of 1000 calls to its API for each GCP project every 100 seconds. After that 100 seconds that limit is reset. Now, very important point, this doesn't limit the rate of calls to your applications running in GKE, but rather calls to the administrative configuration of your GKE clusters themselves. It would be very unusual to make that many calls in such a short period of time. And that quota might have helped catch and stop that erroneous behavior. 

**Allocation quotas** govern the number of resources you can have in your projects. This doesn't reset at any interval. Instead, you need to free up those resources to stay within them. For example, by default each GCP project has a quota allowing it no more than five VPC networks or virtual project clouds. These quotas are not the same for all projects. Although project start with the same quotas, you can change some of them by requesting an increase from Google Cloud support. Some quotas may increase automatically based on your usage of a product. And you can use the GCP console to explicitly lower some of them for your own projects. For example, if you want to put a more stringent cap on your consumption. Finally, some quotas are fixed for all GCP customers regardless. In addition to the benefits to customers, GCP quotas also protect the community of GCP users by reducing the overall risk of unforeseen spikes in usage.

## Interacting with Google Cloud

This next video looks at how you interact with GCP. You learned about the Google tools and interfaces that allow you to manage and configure your GCP resources. There are four ways to interact with GCP. The Google Cloud Platform Console, the cloud shell in Cloud SDK, the Cloud Console, mobile app and REST based APIs. Now we're not going to focus very much on APIs in this specialization, developers use them to build applications that allocate and manage GCP resources, but our present focus is on letting Kubernetes manage those resources for us. The GCP console is a web based graphical user interface from where you manage your GCP resources. It allows you to execute common tasks using simple mouse clicks with no need to remember commands and avoiding typos. It also provides visibility into your GCP project an its resources. Now you can sign into the GCP console from a web browser at console dot cloud dot Google dot com. All GCP services are accessible through the simple menu button in the top left hand corner. You can pin frequently used services to this menu. You'll learn how to use the GCP console during an upcoming lab. Alternatively, you can download and install the Google Cloud SDK onto a computer of your choice. The Cloud SDK contains a set of command line tools for the Google Cloud platform. Most notably for us it contains the G Cloud and the kubectl commands which we will use a lot of in this course. It also contains the gsutil and bq utilities. You can run these tools interactively or in your automated scripts. The Cloud SDK contains client libraries for various programming languages too. But what if it isn't convenient to install the cloud SDK on the machine that you're working with? Cloud Shell provides command line access to your cloud resources directly from within your web browser. Using Cloud Shell you can manage your projects and resources easily without having to install the Cloud SDK or other tools locally. The Cloud SDK, G Cloud and kubectl command line tools and other utilities are always available up to date and fully authenticated. So how does Cloud Shell do that? It's built using a compute engine virtual machine instance that you're not billed for. Each GCP user has one. Your cloud shell virtual machine is ephemeral, which means it will be stopped whenever you stop using it interactively. And it'll be restarted for you every time you re enter cloud shell, so you wouldn't want to run a production web server in your cloud shell for example. You'll also get five gigabytes of persistent disk storage that is reattached for you every time a new cloud shell session is started. It also provides a web preview functionality and built in authorization process for access to GCP console projects and resources, including your GKE resources. The cloud shell code editor is a tool for editing files inside of your cloud shell environment in real time within your web browser. You can also use text editors from within the Cloud Shell command prompt. This tool is extremely convenient when working with code first applications or container based workloads because you can easily edit files without the need to download and upload changes. But is the easy way always the best way? Of course not, later on in this specialization we'll talk about the best management practices for these code files. Here's a screenshot of the GCP console's GKE area, showing you its web based interface for administering your GKE resources. The bottom third of the screenshot is your cloud shell in operation where you can launch commands to administer those resources as well. Some of those commands are from the Google cloud SDK and others will be specific to your workload. Later in this course we'll learn about the kubectl command and you can see it being launched from cloud shell here. Finally, there's the Cloud Console mobile app available for iOS and Android. It offers many capabilities like managing virtual machines and viewing their logs, getting up to date billing information for your projects, getting billing alerts for projects that are going over budget and setting up customizable graphs showing key metrics such as CPU usage, network usage, requests per second in any server.

# Containers and Kubernetes

## Introduction to Containers and Container Images

### Introduction to Containers

Let's start by introducing Containers, and this video, you'll learn about the key features of Containers and the advantages of using Containers for application deployment compared to alternatives such as deploying apps directly to Virtual Machines. You'll learn about Google's Cloud build, and then you can see how you can use it to build a manager application Images. It's now not very long ago, the default way to deploy an application was on its own physical computer. To set one up, you'd find some physical space, power, cooling, network connectivity for it, and then install an operating system, any software dependencies, and then finally the application itself, and if you need more processing power, redundancy, security or scalability, what you do, well, you'd have to simply add more computers. It is very common for each computer to have a single-purpose, for example, database, web server, or content delivery. This practice, as you might imagine, wasted resources and it took a lot of time to deploy and maintain and scale. It also wasn't very portable at all. Applications were built for a specific operating system and sometimes even for specific hardware as well. In comes, the dawn of virtualization. Virtualization helped by making it possible to run multiple virtual servers and operating systems on the same physical computer. A hypervisor is the software layer that breaks the dependencies of an operating system with its underlying hardware and allow several Virtual Machines to share that same hardware. KVM as one well-known hypervisor. Today you can use virtualization to deploy new servers fairly quickly. Now adopting virtualization means that it takes less time to deploy new solutions. We waste less of the resources on those physical computers that we're using, and we get some improved portability because Virtual Machines can be imaged and then moved around. However, the application, all of its dependencies and operating system are still bundled together, and it's not very easy to move from a VM from one hypervisor product to another, and every time you start up a VM, it's operating systems still takes time to boot up. Running multiple applications within its single VM also creates another tricky problem. Applications that share dependencies are not isolated from each other. The resource requirements from one application can starve out other applications of the resources that they need. Also a dependency upgrade for one application might cause another to simply stop working. You can try to solve this problem with rigorous software engineering policies. For example, you could lock down the dependencies that no application is allowed to make changes. But this leads to new problems because dependencies do need to be upgraded occasionally. You can't add integration tests to ensure that applications were. Integration tests are great. But dependency problems can cause new failure modes that are hard to troubleshoot, and it really slows down development if you have to rely on integration tests to simply just perform basic integrity checks of your application environment. Now, the VM centric way to solve this problem is to run a dedicated Virtual Machine for each application. Each application maintains its own dependencies and the kernel is isolated. So one application won't affect the performance of another, and what you can get, as you can see here, is two complete copies of the kernel that are running. But here too, we can run into issues is you're probably thinking. Scale this approach to hundreds of thousands of applications and you can quickly see the limitation. Just imagine trying to do a simple kernel update. So for large systems, dedicated VMs are redundants and wasteful. VMs are also relatively slow to startup because the entire operating system has to boot. More official way to resolve the dependency problem is to implement abstraction at the level of the application, and as dependencies. You don't have to virtualize the entire machine or even the entire operating system, but just the User Space, and again, the User Space is all the code that resides above the kernel and includes the applications and their dependencies. This is what it means to create Containers. Containers are isolated user spaces per running application code. Containers are lightweight because they don't carry a full operating system that can be scheduled are packed tightly onto the underlying system, which is very efficient. It can be created and shut down very quickly because you're just starting and stopping the processes that make up the application and not booting up an entire VM and initializing an operating system for each application. Developers appreciate this level of abstraction because they don't want to worry about the rest of the system. Containerization is the next step in the evolution of Managing code. You now understand Containers as delivery vehicles for application code. The lightweight, stand-alone, resource efficient, portable execution packages. You develop application code in the usual way. On desktops, laptops, and servers. The Container allows you to execute your final code and VMs without worrying about software dependencies like application runtimes, system tools, system libraries, and other settings. You package your code with all the dependencies it needs and the engine that executes your container is responsible for making them available at runtime. Containers appeal to developers because there are an application centric way to deliver high-performing and scalable applications. Containers also allow developers to safely make assumptions about the underlying hardware and software. With a Linux kernel underneath, you no longer have code that works on your laptop, but doesn't work in production. The Container is the same and runs the same anywhere. You make incremental changes to Container base on it production image, you can deploy it very quickly with a single file copy. This speeds up your development process. Finally, Containers make it easier to build applications that use the microservices design pattern that is loosely coupled, fine-grained components. This modular design pattern allows the operating system to scale and also upgrade components of an application without affecting the application as a whole.

### Containers and Container Images

An application and its dependencies are called an image. A container is simply a running instance of an image. By building software into container images, developers can easily package and ship an application without worrying about the system it will be running on. You need software to build container images, and surround them. Docker is one tool that does both. Docker is an open source technology that allows you to create and run applications in containers, but it doesn't offer a way to orchestrate those applications at scale like Kubernetes does. In this course will use Google's Cloud Build to create Docker formatted container images. Containers are not an intrinsic primitive feature of Linux. Instead their power to isolate workloads is derived from the composition of several technologies. One foundation is the Linux process. Each Linux process has its own virtual memory address space, separate from all others, and Linux processes are rapidly created and destroyed. Containers use Linux namespaces to control what an application can see, process ID numbers, directory trees, IP addresses and more. By the way, Linux namespaces are not the same thing as Kubernetes namespaces, which we'll learn more about later on in this course. Containers use Linux, cgroups to control what an application can use. Its maximum consumption of CPU time, memory, IO bandwidth and other resources. Finally, containers use union filesystems to efficiently encapsulate applications and their dependencies into a set of clean minimal layers. Now let's see how that works. A container image is structured in layers. The tool you use to build the image reads instructions from a file called the container manifest. In the case of a Docker formatted container image, that's called a Dockerfile. Each instruction in the Dockerfile, specifies a layer inside the container image, each layer is read-only. When it container runs from this image, it will also have a writable Ephemeral topmost layer. Let's take a look at his simple Dockerfile. This Dockerfile will contain four commands, each of which creates a layer. At the end of this discussion, I'll explain why this Dockerfile is a little oversimplified for modern use. The from statement starts out by creating a base layer, pulled from a public repository. This one happens to be the Ubuntu Linux runtime environment of the specific version. The copy command adds a new layer, containing some files copied in from your build tools current directory. The run command builds your application using the make command, and puts the result of the build into a third layer. And finally the last layer specifies what command to run within the container, when it's launched. Each layer is only a set of differences from the layer before it. When you write a Dockerfile, you should organize the layers least likely to change, through to the layers that are most likely to change. By the way, I promised that I'd explain how the Dockerfile example you saw here is oversimplified. These days the best practice is not to build your application in the very same container that you ship and run. After all, your build tools are at best just cluttering in a deployed container, and at worst an additional attack service. Today application packaging relies on a multi stage build process, in which one container builds the final executable image, and a separate container receives only what's needed to actually run the application. Fortunately for us, the tools that we use support this practice. When you launch a new container from an image, the container runtime adds a new writable layer on the top of the underlying layers. This layer is often called the container layer. All changes made to the running container, such as writing new files, modifying existing files, and deleting files, are written to this thin, writable container layer and they're ephemeral. When the container is deleted, the contents of this writable layer are lost forever. The underlying container image itself remains unchanged. This fact about containers, has an implication for your application design. Whenever you want to store data permanently, you must do so somewhere other than a running container image. You'll learn that in the several choices that you can choose from, in this specialization. Because each container has its own writable container layer, and all changes are stored in this layer, multiple containers can share access to the same underlying image, and yet have their own data state. The diagram here shows multiple containers sharing the same Ubuntu 1504 image. Because each layer is only a set of differences from the layer before it, you get smaller images. For example, your base application image maybe 200 megabytes, but the difference to the next point release, might only be 200 kilobytes. When you build a container, instead of copying the whole image, it creates a layer with just the differences. When you run a container, the container runtime pulls down the layers it needs. When you update, you only need to copy the difference. This is much faster than running a new virtual machine. It's very common to use publicly available open source container images, as a base for your own images, or for unmodified use. For example, you've already seen the Ubuntu container image, which provides an Ubuntu Linux environment inside of a container. Alpine is a popular Linux environment in a container, noted for being very, very small. The Nginx web server is frequently used in its container packaging. Google maintains a container registry gcr.io. This registry contains many public open source images, and Google Cloud customers also use it to store their own private images, in a way that integrates well with cloud.im. Google Container Registry is integrated with cloud.im, so for example you can use it to store your images that aren't public, instead they're private to your project. You can also find container images in other public repository's, Docker Hub Registry, GitLab and others. The open source Docker command is a popular way to build your own container images. It's widely known and widely available. One downside, however, of building containers with a Docker command, is that you must trust the computer that you do your builds on. Google provides a managed service for building containers, that's also integrated with cloud.im. This service is called Cloud Build, and we'll use it in this course. Cloud Build can retrieve the source code for your builds, from a variety of different storage locations, cloud source repositories cloud storage, which is GCP object storage service or git compatible repository's like GitHub and Bitbucket. To generate a build with Cloud Build, you define a series of steps. For example, you can configure build steps to fetch dependencies, compile source code, run integration tests, or use tools such as Docker, Gradle, and Maven. Each build step in Cloud Build, runs in a docker container. Then Cloud Build can deliver your newly build images to various execution environments, not only GKE, but also App Engine and Cloud Functions.

## Introduction to Kubernetes

Now let's introduce a popular container management and orchestration solution called Kubernetes. Let's say your organization has really embraced the idea of containers. Because containers are so lean, your coworkers are creating them in numbers far exceeding the counts of virtual machines you used to have. The applications running in them need to communicate over the network. But you don't have a network fabric that lets containers find each other. You need help. How can you manage your container infrastructure better? Kubernetes is an open source platform that helps you orchestrate and manage your container infrastructure On-premises or in the Cloud. So what is Kubernetes? It's a containers centric management environment. Google originated it and then donated it to the open source community. Now it's a project of the vendor-neutral Cloud Native Computing Foundation. It automates the deployment, scaling, load balancing, logging, monitoring, and other management features of containerized applications. These are the features that are characteristic of a typical Platform-as-a-Service solutions. Kubernetes also facilitates the features of an Infrastructure-as-a-Service, such as allowing a wide range of user preferences and configuration flexibility. Kubernetes supports declarative configurations. When you administer your infrastructure declaratively, you describe the desired state you want to achieve, instead of issuing a series of commands to achieve that desired state. Kubernetes' job is to make the deployed system conform to your desired state and then keep it there in spite of failures. Declarative configuration saves you work because the system's desired state is always documented. It also reduces the risk of error. Kubernetes also allows imperative configuration in which you issue commands to change the system state. But administering Kubernetes at scale imperatively will be a big missed opportunity. One of the primary strengths of Kubernetes is its ability to automatically keep a system in a state that you declare. Experienced Kubernetes administrators use imperative configuration only for quick temporary fixes and as a tool in building a declarative configuration. Now that you know what Kubernetes is, let's talk about some of its features. Kubernetes supports different workload types. It supports stateless applications, such as an NGINX or Apache web server and stateful applications, where user in session data can be stored persistently. It also supports batch jobs and daemon tasks. Kubernetes can automatically scale in and out containerized applications based on resource utilization. You can specify resource request levels and resource limits for your workloads and Kubernetes will obey them. These resource controls like Kubernetes improved overall workload performance within a cluster. Developers extend Kubernetes to a rich ecosystem of plugins and add-ons. For example, there's a lot of creativity going on currently with Kubernetes custom resource definitions, which bring the Kubernetes declarative management model to amazing variety of other things that need to be managed. The primary focus of this specialization though is architecting with Kubernetes. Because it's provided as a service by Google Cloud, so extending Kubernetes is not within our scope. Because it's open source, Kubernetes also supports workload portability across On-premises or multiple Cloud service providers such as GCP and others. This allows Kubernetes to be deployed anywhere. You can move Kubernetes workloads freely without vendor lock-in.

## Introduction to Google Kubernetes Engine

Google cloud's managed service offering for Kubernetes is called Google Kubernetes engine or GKE. So why do people choose it? What if you begun using Kubernetes in your environment, but the infrastructure has become too much of a burden for you to maintain? Is there anything within Google Cloud Platform that can help you? Absolutely totally, that's going to be Google Kubernetes Engine or GKE. GKE, let's talk about what it can do. It will help you deploy, manage, and scale a Kubernetes' environments for your containerized applications on GCP. More specifically, GKE is a component of the GCP, compute offerings, it makes it easy to bring your Kubernetes workloads into the cloud. GKE is fully managed, which means that you don't have to provision the underlying resources. GKE uses a container optimized operating system, these operating systems are maintained by Google, and are optimized to scale quickly, and with a minimal resource footprint. The container optimized OS, will be discussed later on in this course. When you use GKE, you start by directing the service to instantiate a Kubernetes system for you, this system, is called a cluster. GKE's auto upgrade feature can be enabled to ensure that your clusters or automatically upgraded with the latest and greatest version of Kubernetes. The virtual machines that host your containers inside of a GKE cluster, are called nodes. If you enable GKE's auto repair feature, the service will automatically repair unhealthy nodes for you. It will make periodic health checks on each node in the cluster, if a node is determined to be unhealthy and requires repair, GKE will drain the node. In other words, it will cause it's workloads to gracefully exit, and then recreate that node. Just as Kubernetes support scaling workloads, GKE supports scaling the cluster itself. GKI seamlessly integrates with Google Cloud build, and container registry. This allows you to automate deployment using private container images, that you've securely stored in container registry. GKE also integrates with Google's identity and access management, which allows you to control access through the use of accounts and role permissions. Stackdriver is Google Cloud system for monitoring and management for services, containers, applications, and infrastructure. GKE integrates with stackdriver monitoring, to help you understand your applications performance. GKE is integrated with Google or virtual private clouds or VPCs, and makes use of GCP's networking features. And finally, the GCP console provides insights into GKE clusters and the resources, and it allows you to view, inspect and delete resources in those clusters. You might be aware that open source Kubernetes contains a dashboard, but it takes a lot of work to set it up securely. But the GCP console is a dashboard for your GKE clusters, and workloads that you don't have to manage, and it's more powerful than the Kubernetes dashboard.

## Compute Options in Detail

In this last lesson, you'll learn more about the computing options available. In a previous module, I briefly introduce your choices for running compute workloads in GCP. Now that we know more about how containers work, we can compare these choices in more detail. The services are Compute Engine, GKE, App Engine, Cloud Run, and Cloud Functions. At the end of this lesson, you'll understand why people choose each. Compute Engine offers Virtual machines that run on GCP. You can select predefined VM configurations. At the time this course was developed, these Virtual Machines can be as large as a 160 VCPUs with more than three terabytes of memory. You can also create customized configurations to precisely match your performance and cost requirements. Virtual Machines need block storage. 

Compute Engine offers you two main choices. Persistent disks and local SSDs. Persistent disks offer network stores that can scale up to 64 terabytes and you can easily take snapshots of these disks for backup and mobility. You can also choose local SSDs, which enable very high input, output operations per second. You can place your Compute Engine workloads behind global load balancers that support Autoscaling. Compute Engine offers a feature called managed instance groups. With these, you can define resources that are automatically deployed to meet demand. GCP enables fine-grained control of costs of Compute Engine resources by providing per second billing. This granularity helps reduce your costs when deploying compute resources for short periods of time, such as batch processing jobs. Compute Engine offers preemptible Virtual Machines, which provides significantly cheaper pricing for your workloads that can be interrupted safely. Why do people choose Compute Engine? With Compute Engine, you have complete control over your infrastructure. You can customize operating systems and even run applications that rely on a mix of operating systems. You can easily lift and shift your on-premises workloads into GCP without rewriting your applications or making any changes. Compute Engine is the best option when other computing options don't support your applications or requirements. 

App Engine has a completely different orientation from Compute Engine. App Engine is a fully managed application platform. Using App Engine means zero server management and zero configuration deployments. If you're a developer, you can focus on building applications and not really worrying about the Deployment part. You can simply use your code and App Engine will deploy that required infrastructure for you. App Engine supports popular languages like Java and Node.js, Python, PHP, C-sharp, dot net, Ruby and go. You can also run Container workloads. Stack driver monitoring, logging and diagnostics such as debugging and Error reporting are also tightly integrated with App Engine. You can use Stack drivers real-time debugging features to analyze and debug your source code. Stack driver integrates with tools such as Cloud SDK, Cloud Source Repositories, Intelligent, Visual Studio, and PowerShell. App Engine also supports a version control and traffic splitting. 

App Engine is a good choice if you simply want to focus on writing code and you don't want to worry about building the highly reliable and scalable infrastructure that are run on, you can just focus on building applications instead of deploying and managing the environment. Use cases for App Engine include websites, mobile apps, gaming backends, and as a way to present a restful API to the internet. What's that RESTful API? In short, is an application program interface that resembles the way a web browser interacts with a web server. RESTful APIs are easy for developers to work with and extend and App Engine makes them easy to operate it. Finally, the main topic of this course, Google Kubernetes Engine. We learned that communities is an orchestration system for applications in containers. It automates deployment, scaling, load balancing, logging and monitoring, and other management features. Google Kubernetes Engine extends Kubernetes management on GCP by adding features and integrating with other GCP services automatically. GKE supports cluster scaling, persistent disks, automated updates to the latest version of Kubernetes and auto repair for unhealthy nodes. It has built-in integration with Cloud build, Container Registry, Stack driver monitoring and Stack driver Logging. Existing workloads running with an on-premise clusters can easily be moved onto GCP. There's no vendor login. Overall, GKE is very well suited for containerized applications. Cloud native distributed systems in hybrid applications. Kubernetes and GKE are discussed in depth throughout this course. Cloud Run is a managed Compute platform that enables you to run stateless Containers via web requests or Cloud Pub Sub events. 

Cloud Run is serverless. It abstracts away all the infrastructure management, so you can focus on developing applications. It's built on Knative, an open source Kubernetes-based platform. It builds, deploys, and manages modern serverless workloads. Cloud Run gives you the choice of running your containers either fully managed or in your own GKE cluster. Cloud Run enables you to run request or event-driven stateless workloads without having to worry about servers. It abstracts away all the infrastructure management, such as provisioning, configuring, managing those servers, so you can focus on just writing code. It automatically scales up and down from zero depending upon traffic almost instantaneously, so you never have to worry about scale configuration. Cloud Run charges you for only the resources that you use, calculated down to the nearest 100 milliseconds, so you never have to pay for those over-provisioned resources. With Cloud Run, you can choose to deploy your stateless containers with a consistent developer experience to a fully managed environment or to your own GKE cluster. This common experience is enabled by Knative, an open API and runtime environment built on top of Kubernetes, and it gives you the freedom to move your workloads across different environments and platforms that are fully managed on GCP, on GKE, or anywhere Knative runs. Cloud Run enables you to deploy stateless containers that listen for requests or events delivered via HTTP requests. With Cloud Run, you can build your applications in any language using whatever frameworks and tools you wish, and deploy them in seconds, without having to manage and maintain that server infrastructure.

Cloud Functions is an event-driven serverless compute service for simple, single-purpose functions that are attached to events. In Cloud Functions you simply upload your code written in JavaScript or Python or Go, and then GCP will automatically deploy the appropriate computing capacity to run that code. These servers are automatically scaled and are deployed from highly available and fault tolerant design. You're only charged for the time that your code runs. For each function, invocation memory and CPU use is measured in the 100 millisecond increments, rounded up to the nearest increment. Cloud Functions also provide a perpetual free tier, so many Cloud Functions use cases could be free of charge. With Cloud Functions, your code is triggered within a few milliseconds based on events. For example, all files uploaded to Google Cloud Storage or a message received from Cloud Pub/Sub. Cloud Functions can also be triggered based on HTTP endpoints that you define and events in the Firebase mobile application backend. What are some of the use cases for Cloud Functions? Their general use is part of microservices application architecture. You could also build simple serverless mobile IoT backends or integrate with third-party services and APIs. Files uploaded into your GCS bucket can be processed in real-time. Similarly, the data can be extracted, transformed, and loaded for querying and analysis. GCP customers often use Cloud Functions as part of intelligent applications, such as virtual assistants, video or image analysis, and sentiment analysis.

Which computer service should you adopt? A lot depends on where you're coming from. If you're running applications on physical server hardware, it will be the path of least resistance to move into Compute Engine. What if you're running applications in long-lived virtual machines in which each VM is managed and maintained? In this case, you'll also find moving to Compute Engine is the quickest GCP service for getting applications to the Cloud. What if you don't want to think about operations at all? Well, App Engine and Cloud Functions are good choices. You can learn more about the differences between App Engine and Cloud Functions in the courses under developing applications with Google Cloud Platform. I hope that this course so far has helped you understand why software containers are so beneficial. Containerization is the most efficient and portable way to package up an application. The popularity of containerization is growing very fast. In fact, both Compute Engine and App Engine can launch containers for you. Compute Engine will accept a container image from you and launch a virtual machine instance that contains it. You can use Compute Engine technologies to scale and manage the resulting VM. App Engine with flexible environment will accept a container image from you, and then run it with the same no-ops environment that App Engine delivers for code. But what if you want more control over your containerized workloads than what App Engine offers, and denser packing what Compute Engine offers? That increasingly popular use case is what GKE is designed to address. The Kubernetes paradigm of container orchestration is incredibly powerful and it's vendor-neutral, and a broad and vibrant community is developed all around it. Using Kubernetes as a managed service from GCP, saves you work and lets you benefit from all the other GCP resources, too. You can also choose Cloud Run to run stateless containers on a managed compute platform. Of course, if you're already running Kubernetes in your on-premises data centers, moving to GKE is a great choice, because you'll be able to bring along both your workloads and your management approach.

# Kubernetes Architecture

## Kubernetes Concepts

Hi, I'm on, in this lesson we'll lay out the fundamental components of the Kubernetes operating philosophy. To understand how Kubernetes works, there are two related concepts you need to understand. The first is Kubernetes object model. Each thing Kubernetes manages is represented by an object. And you can view and change these object's attributes and state.

The second is the principle of declarative management. Kubernetes expects you to tell it what you want the state of the objects under its management to be. And it will work to bring that state into being and keep it there. How does it do that? By means of its so called watch loop. Formerly a Kubernetes object is defined as a persistent entity that represent the state of something running in a cluster, its desired state, and its current state.

Various kinds of objects represent containerized applications, the resources that are available to them, and the policies that affect their behavior. Kubernetes objects have two important elements.

You give Kubernetes an object spec for each object you want to create. With the spec, you define the desired state of the object by providing the characteristics that you want. The object status is simply the current state of the object provided by the Kubernetes control plane. By the way, we use the term Kubernetes control plane to refer to the various system processes that collaborate to make a Kubernetes cluster work. You'll learn more about these processes later in this module. Each object is of a certain type or kind as Kubernetes castle. Pods are the basic building block of the standard Kubernetes module. And they are the smallest deployable Kubernetes object.

You may be surprised to hear me say that, maybe you're expecting me to say the smallest Kubernetes object is a container. Not so.

Every running container in a Kubernetes system is a pod.

A pod embodies the environment where containers live and that environment can accommodate one or more containers. If there is more than one container in a pod, they are tightly coupled, and they share resources, including networking and storage.

Kubernetes assigns each pod a unique IP address. Every container within a pod shares the network name space, including IP address and net reports.

Containers within the same pod can communicate through local host 127.0.0.1. A pod can also specify a set of storage volumes to be shared amongst its containers.

By the way, later in the specialization, you'll learn how pods can share storage with one another and not just a single pod. Let's consider a simple example where you want three instances of the Nginx Web server each of its own container running all the time. How is this achieved in Kubernetes?

Remember, the Kubernetes embodies the principle of the clarity of management. You declare some objects to represent those in Nginx containers. What kind of object? Perhaps pods. Now it is Kubernetes job to launch those pods and keep them in existence. But be careful, pods are not self-healing. If we want to keep all our Nginx Web servers not just in existence but also working together as a team, we might want to ask for them using a more sophisticated kind of object. We'll tell you more about this later in the module.

Let's suppose we have given Kubernetes a desired state that consists of three Nginx pods always kept running.

We did this by telling Kubernetes to create and maintain one or more objects that represent them. Now Kubernetes compares the desired state to its current state. Let's imagine that our declaration of three Nginx containers is completely new. The current state does not match the desired state. So Kubernetes specifically its control plane will remedy the situation. Because the number of desired pods running for the object we declared is three and zero are presently running, three will be launched. And the Kubernetes control plane will continuously monitor the state of the cluster, endlessly comparing reality to what has been declared and remedying state as needed.

## The Kubernetes Control Plane 

0:00
In the previous lesson, I mentioned the Kubernetes control plane, which is the fleet of cooperating processes that make a kubernetes cluster work.

Even though you'll only work directly on a few of these components, it helps to know about them and the role each plays.

I'll build up a kubernetes cluster, part by part, explaining each piece as I go. After I'm done, I'll show you how a Kubernetes cluster running in GKE is a lot less work to manage the one you provision yourself. Okay, here we go. First and foremost, your cluster needs computers. Nowadays, there are computers to compose your clusters. Usually our virtual machines. They always are in GKE but they could be physical computers, too. One computer is called the control plane and the others are simply called nodes.

The job of the nodes is to run parts.

The job of the control plane is to coordinate the entire cluster. We'll meet its control playing components first.

Several critical kubernetes components run under control plane. The single component that you interact with directly is called the kube-APIserver. This component's job is to accept commands that view or change the state of the cluster, including launching pods. In the specialization, you will use the kubectl command frequently. This command's job is to connect to the kubeAPIserver and communicate with us using the kubernetes API. KubeAPIserver also authenticates incoming requests, determines whether they are authorized and valid, and managers admission control. But it's not just the kubectl that talks with kube-APIserver.

In fact, any query or change toward the cluster state must be addressed to the kubeAPIserver. Etcd is the clusters database. Its job is to reliably store the state of the cluster. This includes all of the cluster configuration data and more dynamic information, such as what nodes are part of the cluster, what pods should be running and where they should be running. You'll never interact directly with etcd. Instead, kube-APIserver interacts with the database on behalf of the rest of the system. Kube-scheduler is responsible for scheduling pods onto nodes. To do that, it evaluates the requirements of each individual pod and selecting which node is most suitable. But it doesn't do the work of actually launching the pods on the nodes. Instead, whenever it discovers a pod object that doesn't yet have an assignment to a node, it chooses a node and simply writes the name of that node into the pod object. Another component of the system is responsible, then for launching the the pods, and you'll see it very soon.

But how does kube-scheduleer decide where to run the pod? It knows the state of all of the nodes, and it will obey constraints that you define on where a pod may run. Based on hardware, software and policy. For example, you might specify a certain pod is only allowed to run on nodes with a certain amount of memory. You can also define affinity specifications, which calls groups of pods to prefer running on the same node. Or anti affinity specifications, which ensure that pods do not run on the same node. You will learn more about some of these tools in later modules. Kube controller manager has a broader job. It continuously monitors the state of the cluster through kube-APIserver. Whenever the current state of the cluster doesn't match the desired state, kube controller manager will attempt to make changes to achieve the desired state. It's called the controller manager because many kubernetes objects are managed by loops of code called controllers. These loops of code handle the process of remediation. Controllers will be very useful for you. To be specific, you'll learn to use certain kinds of kubernetes controllers to manage workloads. For example, remember our problem of keeping three engine x pods always running.

We can gather them together into a controller object called a deployment. That not only keeps them running, but also let's us scale them and bring them together under the front end. We'll meet deployments later in this module. Other kinds of controllers have system level responsibilities. For example, node controller's job is to monitor and respond when a node is offline. Kube cloud manager manages controllers that interact with the underlying cloud providers. For example, if you manually launch a kubernetes cluster on Google Compute Engine, kube cloud manager would be responsible for bringing in Google cloud features like load balancers and storage volumes when you needed them. Each node runs a small family of control plane components too. For example, each node runs a kubelet. You can think of a kubelet as kubernetes agent on each node. When the kube-APIserver wants to start a pod on a node, it connects to that node's kubelet. Kubelet uses the container runtime to start the pod and monitors its life cycle. Including readiness and liveliness probes and reports back to kube-APIserver. Do you remember our use of the term container runtime in the previous module? This is the software that knows how to launch a container from a container image.

The world of kubernetes offers several choices for container run times. But the Linux distribution that GKE uses for its node launches containers using Container D. >> The runtime component of Docker. Kube-proxies job is to maintain the network connectivity among the pods in a cluster. In open source kubernetes, it does so using the firewall capabilities of IP tables, which are built into the Linux kernel. Later, in the specialization, we will learn how GKE handles part networking

## Google Kubernetes Engine Concepts

Next, we'll introduce concepts specific to Google Kubernetes Engine. That diagram of the control plane had a lot of components, didn't it? Setting up a Kubernetes cluster by hand is a lot of work. Fortunately, there is an open-source command called kubeadm that can automate much of the initial setup of a cluster, but if a node fails or needs maintenance, a human administrator has to respond manually. I only suspect you can see why many people like the idea of a managed service for Kubernetes. You may be wondering how that picture we just saw differs from GKE. Well, here it is. From a user's perspective, it's a lot simpler. GKE manages all of the control plane components for us. It still exposes an IP address to which we send all of our Kubernetes API requests, but GKE takes the responsibility for provisioning and managing all of the control plane infrastructure behind us. It also abstracts away having a separate control plane. The responsibilities of the control plane are absorbed by Google Cloud, and you are not separately billed for your control plane. Now, let's talk about nodes. In any Kubernetes environment, nodes are created externally by cluster administrators, not by Kubernetes itself. GKE automates this process for you. It launches Compute Engine virtual machine instances and registers them as nodes. You can manage nodes settings directly from the Cloud Console. You pay per hour of life of your nodes, not counting the control plane. Because nodes run a Compute Engine, you can choose your node machine type when you create your cluster. By default, the node machine type is n1 standard-1, which provides one virtual CPU and 3.75 gigabytes of memory. Google Cloud, offers a wide variety of Compute Engine options. At the time this video was made, the generally available maximum was 96 virtual CPU cores. That's a moderately big virtual machine. You can customize your nodes, the number of cores, and their memory capacity. You can select a CPU platform. You can choose a baseline minimum CPU platform for the nodes or node pool. This allows you to improve node performance. GKE will never use a platform that is older than the CPU platform you specify. If it picks a newer platform, the cost will be the same as the specified platform. You can also select multiple node machine types by creating multiple node pools. A node pool is a subset of nodes within a cluster that share a configuration, such as the [inaudible] memory or the CPU generation. Node pools also provide an easy way to ensure that workloads run on the right hardware within your cluster, you just label them with a desired node pool. By the way, node pools are a GKE feature rather than a Kubernetes feature. You can build an analogous mechanism within open-source Kubernetes, but you'd have to maintain it yourself. You can enable automatic node of crates, automatic node repairs, and cluster auto-scaling at this node pool level. Here's a word of caution, some of each node CPU and memory are needed to run the GKE and Kubernetes components that let it work as part of your cluster. For example, if you allocate nodes with 15 gigabytes of memory, not quite all of that 15 gigabytes will be available for use by pods. This module has a documentation link that explains how much CPU and memory are reserved. By default, a cluster launches on a single Google Cloud Compute zone with three identical nodes, all on one node pool. The number of nodes can be changed during or after the creation of the cluster. Adding more nodes and deploying multiple replicas of an application will improve an application's availability, but only up to a point. What happens if an entire Compute zone goes down? You could address this concern by using a GKE regional cluster. Regional clusters have a single API endpoint for the cluster. However, its control planes and nodes are spread across multiple Compute Engine zones within a region. Regional clusters ensure that the availability of the application is maintained across multiple zones in a single region. In addition, the availability of the control plane is also maintained so that both application and management functionality can withstand the loss of one or more but not all zones. By default, a regional cluster is spread across three zones, each containing one control plane and three nodes. These numbers can be increased or decreased. For example, if you have five nodes in zone one, you will have exactly the same number of nodes for each of the other zones for a total of 15 nodes. Once you build a zonal cluster, you cannot convert it into a regional cluster or vice versa. A regional or zonal GKE cluster can also be set up as a private cluster. The entire cluster, that is the control plane and it's nodes, are hidden from the public Internet. Cluster control planes can be accessed by Google Cloud products such as Cloud Logging or Cloud monitoring through an internal IP address. They also can be accessed by authorized networks through external IP address. Authorize networks are basically IP address ranges that are trusted to access the control plane. In addition, nodes can have a limited outbound access through private Google access, which allows them to communicate with other Google Cloud services. For example, nodes can pull Container images from Google Container Registry without needing external IP addresses. The topic of private clusters is discussed in more detail in another module in this specialization.

## Kubernetes Object Management

Now we'll discuss Kubernetes object management. All Kubernetes objects are identified by a unique name and a unique identifier.

Let's return once again to our example in which we want three nginx web servers running all the time. Well, the simplest way would be to declare three pod objects and specify their state. For each, a pod must be created and an nginx container image must be used. Let's see how we declare this.

You define the objects you want Kubernetes to create and maintain with manifest files. These are ordinary text files. You may write them in YAML or JSON format. YAML is more human, readable and less tedious to edit, and we will use it throughout this specialization. This YAML file defines a desired state for a pod, its name and a specific container image for it to run.

Your manifest files have certain required fields. API version describes which Kubernetes API version is used to create the object. The Kubernetes protocols version so as to help maintain backwards compatibility.

Kind defines the object you want, in this case, a pod. And metadata helps identify the object using name, unique ID, and an optional namespace. You can define several related objects in the same YAML file, and it is a best practice to do so. One file is often easier to manage than several. Another even more important tip. You should save your YAML files in version controlled repositories. This practice makes it easier to track and manage changes, and to back out those changes when necessary. It's also a big help when you need to recreate or restore a cluster. Many GCP customers use cloud source repositories for this purpose because that service lets them control the permissions of those files in the same way as their other GCP resources.

When you create a Kubernetes object, you name it with a string. Names must be unique. Only one object of a particular kind can have a particular name at the same time in a Kubernetes namespace. However, if an object is deleted, its name can be reused. Alpha numeric characters, hyphens and periods are allowed in the names with a maximum character length of 253.

Each object generated throughout the life of a cluster has a unique ID generated by Kubernetes. This means that no two objects will have the same new ID throughout the life of a cluster.

Labels are key value pairs with which you tag your objects during or after their creation. Labels help you identify and organize objects and subsets of objects. For example, you could create a label called app and give as its value the application of which this object is a part.

In this simple example, a deployment object is labeled with three different key values, its application, its environment, and which stack it forms a part of. Various contexts offer ways to select Kubernetes resources by their labels. In this specialization, you will spend plenty of time with the kubectl command. Here's an example of using it to show all the pods that contain a label called app, with a value of nginx. Label selectors are very expressive. You can ask for all the resources that have a certain value for a label, all those that don't have a certain value or even all those that have a value in a set you supply.

So one way to bring three nginx web servers into being would be to declare three pod objects, each with its own section of YAML. Kubernetes default scheduling algorithm prefers to spread the workload evenly across the nodes available to it. So we'd get a situation like this one. Looks good, doesn't it? Maybe not. Suppose I want 200 more nginx instances, managing 200 more sections of YAML sounds very inconvenient. Here's another problem. Pods don't heal or repair themselves, and they're not meant to run forever. They are designed to be ephemeral and disposable. For these reasons, there are better ways to manage what you run in Kubernetes than specifying individual pods. You need to set up like this to maintain an applications high availability, along with horizontal scaling. So, how do you tell Kubernetes to maintain the desired state of three nginx containers?

We can instead declare a controller object whose job is to manage the state of the pods. Some examples of these objects deployments, statefulSets, demon sets and jobs. We'll meet all of these in our specialization. Deployments are a great choice for long lived software components like web servers, especially when we want to manage them as a group. In our example, when kube-scheduler schedules pods for a deployment, it notifies the kube-API server. These changes are constantly monitored by controllers, especially by the deployment controller. The deployment controller will monitor and maintain three nginx pods. If one of those pods fails, the deployment controller will recognize the difference between the current state and the desired state, and will try to fix it by launching a new pod. Instead of using multiple gamble manifests or files for each pod, you used a single deployment YAML to launch three replicas of the same container. A deployment ensures that a defined set of pods is running at any given time. Within its objects spec, you specify how many replica pods you want, how pods should run, which containers should run within these pods, and which volumes should be mounted. Based on these templates, controllers maintain the pods desired state within a cluster.

Deployments can also do a lot more than this, which you will see later in the course.

It's very probable that you'll be using a single cluster for multiple projects. At the same time, it's essential to maintain resource quotas based on projects or teams. By the way, when I say projects here, I mean projects in the informal sense of the word, things you and your colleagues are working on. Each Kubernetes cluster is associated with one GCP project in the formal sense of the word project. And that's how I am policies apply to it and how you're billed for it.

So how do you keep everybody's work on your cluster tidy and organized? Kubernetes allows you to abstract a single physical cluster into multiple clusters known as namespaces. Namespaces provide scope for naming resources such as pods, deployments, and controllers.

As you can see in this example, there are three namespaces in this cluster, test, stage, and prod. Remember that you cannot have duplicate object names in the same namespace. You can create three pods with the same name, nginx in this case, but only if they don't share the same namespace.

If you attempt to create another pod with the same name nginx pod in namespace test, you won't be allowed. Object names need only be unique within a namespace, not across all namespaces. Namespaces also let you implement resource quotas across the cluster. These quotas defined limits for resource consumption within a namespace. They're not the same as your GCP quotas, which we discussed in an earlier module. These quotas apply specifically to the Kubernetes cluster they're defined on. You're not required to use namespaces for your day to day management. You can also use labels. Still, namespaces are a valuable tool. Suppose you want to spin up a copy of a deployment as a quick test. Doing so in a new namespace makes it easy and free of name collisions. There are three initial namespaces in the cluster. The first is a default namespace for objects with no other namespace defined. Your workload Resources will use this namespace by default.

Then there is the kube-system namespace for objects created by the Kubernetes system itself. We'll see more of the object kinds in this diagram elsewhere in this specialization. When you use the kubectl command by default, items in the kube system namespace are excluded, but you can choose to view its contents explicitly.

The third namespace is the kube-public namespace for objects that are publicly readable to all users. Kube-public is a tool for disseminating information to everything running in a cluster. You're not required to use it, but it can come in handy, especially when everything running in a cluster is related to the same goal and needs information in common.

You can apply a resource to a namespace when creating it, using a command line namespace flag, or you can specify a namespace in the YAML file for the resource. Whenever possible, apply namespaces at the command line level. This practice makes your YAML files more flexible. For example, someday you might want to create two completely independent instances of one of your deployments, each in its own namespace. This is difficult if you have chosen to embed namespace names in your YAML files.

## Migrate for Anthos

### Introduction

Hi, I'm Eoin. You've heard a bit about Kubernetes and it sounds pretty great. But what if you've got existing applications that are not in containers or perhaps not even in the Cloud. Don't worry because Google Cloud has a solution for this. Migrate for Anthos is our tool for getting workloads into containerized deployments on Google Cloud. Let's look at what Migrate for Anthos does. Migrate for Anthos moves your existing applications into a Kubernetes environment. The best thing about this, is the process is automated. Your workloads can be On-premises or in other Cloud providers. I've already mentioned that Migrate for Anthos is automated, but it's also very fast. Most migrations are completed in less than ten minutes and you have the choice of Migrating Applications data in one move or Streaming to the Cloud until application is live. That sounds pretty amazing. Let's have a quick overview of what happens in a Migration.

### Architecture

First, let's inspect the architecture required for a migration. The first step is to allow migrate for compute engine to create a pipeline for streaming or migration data from on premises or another cloud provider into Google Cloud. Migrate for compute engine is a tool that allows you to bring your existing applications into VMS on Google Cloud. Migrate for Anthos is then installed on a GKE processing cluster and is composed of many Kubernetes resources. Migrate for Anthos is used to generate deployment artifacts. Some of these artifacts, like your Kubernetes configurations in the dockerfile are used to create the VM wrapping container. This container goes into cloud storage. The container images themselves are stored in the container registry. After deployment assets are created that can be used to deploy your application into a target cluster. You simply apply the generator configuration and it creates all the necessary Kubernetes elements on the target cluster.

### Migration Path

Now that you've seen the architecture required from migration, let's look at what happens when your migration application using Migrate for Anthos. First you need to create the processing cluster. After that you install the Migrate for Anthos components onto that cluster. Next you need to add a migration source. You can migrate from VMware, AWS, Azure or Google cloud. You will need to create a migration object with the details of the migration that you're performing. This will generate a plan template for you in a YAML file. You may need to alter this configuration file to create the level of customization that you desire. When the plan is ready, you will need to generate the artifacts for the migration. This means generation of the container images of your applications on the YAML files required for the deployment. After your migration artifacts have been generated, they need to be tested. Both the container images and the deployments will be tested at this stage. Finally, if the tests are successful, you can use the generated artifacts to deploy your application into your production clusters.

### Installation

Okay, we've looked at the infrastructure required for using Migrate for Anthos. And the basic path that the Migration journey would take. Now let's look at a worked example of installing the necessary tools to perform our Migration. Let's go through an example and see what happens at each stage. The first thing you need to do is setup the processing cluster. Before you run the command on screen, you need to make sure that you are a GKE admin to set up the cluster. You also must have firewall rules in place to allow communications between Migrate for Anthos and Migrate for compute engine. After all that's done, you can create the processing cluster. The example on screen enables a VPC native cluster. When the processing cluster is up and running, you need to install Migrate for Anthos using the MIG CTL command. This command installs all of the required Kubernetes resources on to the processing cluster for the Migration. The mix ETL Source create command specifies the location of the application to Migration. The example on screen is from migrating from Google Compute Engine. If you're migrating from a VM ware back end or another Cloud provider, you need to install some additional packages. Now that the infrastructure elements are setup, the next step is to create a Migration plan. The mix ETL Migration create command will create the Migration plan. This command will define the Migration resources to be created on the cluster. You identified the source VM on what data to exclude from the Migration. You can also specify what Migration intense you want. You can specify the following intense, image, image and data, data, or PV based Container. The output of this command is a YAML file that can be further customized. After creating a Migration plan, you will need to generate the artifacts for the Migration, the mix ETL Migration, generate artifacts command on screen will start this process. This process were first copy files and directories representing the VM to a Container Image Registry as images. Migrate for Anthos creates two images, a runnable image for deployment into another cluster and a non runnable image layer that can be used to update the Container Image in the future. Next, Migrate for Anthos will generate configuration YAML files that you can use to deploy the VM to another GKE cluster. These are copied into a Cloud Storage bucket as an intermediate location. You run the mix ETL Migration get artifacts command to download the YAML configuration files generated from the last step. The YAML configuration defines a resources to deploy. Such as, are you creating a deployment or a [inaudible]? Is the deployment a headless service? Are you using persistent volumes or persistent volume claims? You can edit the YAML file to customize your Deployment. Examples of customizations include enabling load balancing, allowing ingress, or defining decides. Finally, you run the `kubectl apply` command to deploy to defined specification.

# Introduction to Kubernetes Workloads

## The kubectl command

Let's start by discussing the Cube CTL Command Cube. CTL is a utility used by administrators to control kubernetes clusters. You use it to communicate with the Cube API server on your control. Plane Cube CTL transforms your command line entries into API calls that it sends to the Cube API server within your selected kubernetes cluster before it can do any work for you. Cube CTL must be configured with location and credentials of a Cuban eighties cluster. For example, take an administrator who wants to see a list of pods in a cluster after connecting cube CTL to the cluster. With the proper credentials, the administrator can issue the cube CTL get pods. Command Cube CTL converts this into an API call which it sends to the Cube API server through https on the clusters control plane server. The Cube AP server processes the request by querying etc. D. The Cube API server then returns the results to Cube CTL. True https. Finally, Cube CTL interprets the API response and displays the results to the administrator at the command prompt. Before you can use Cube CTL to configure your cluster, you must configure it. First Cube CTL stores its configuration in a file in your home directory in a hidden folder named Dot Cube. The configuration file contains the list of clusters and the credentials that you use to attach to each of those clusters. You may be wondering, where do you get these credentials for G. K E? The service provides them to you through the G Cloud Command. I'll show you how that works in a moment. To view the configuration, you can either open the CONFIG file or use the cube CTL command config view just to be clear here. Cube CTL CONFIG view tells you about the configuration of the Cube CTL Command itself. Other Cube CTL commands tell you about configurations of your clusters and workloads to connect a G K cluster with Q CTL. Retrieve your credentials for the specified cluster first. To do this, use the get credentials G Cloud Command in any other environment where you've installed the G Cloud command line Tool and Cube CTO. Both of these tools are installed by default in the cloud shell. The G cloud get credentials Command writes the configuration information into a config file in the dot cube directory in the home directory by default. If you re run this command for a different cluster, it will update the config file with the credentials for the new cluster, you only need to perform this configuration process once per cluster in your cloud shell because the dot cube directory and its contents stay in your home directory. Can you figure out why the commander's G cloud get credentials rather than Cube CTL credentials? It's because the Cube CTL Command requires credentials to work at all. The G Cloud command is how authorized users interact with Google Cloud from the command line. The G cloud get credentials. Command gives you the credentials you need to connect to a geeky cluster if you're authorized to do so. In general. Q. CTO is a tool for administering the internal state of an existing cluster, but Q C T l can't create new clusters or change the shape of existing clusters. For that, you'll need the geeky control plane, which the G Cloud Command and the cloud console are your interfaces to. Once the CONFIG file in the dot Cube folder has been configured, the Cube CTL Command automatically references this file and connects you to the default cluster without prompting for your credentials. Now let's talk about how to use the Cube CTL Command. Its syntax is composed of several parts. The command, the type, the name and optional flags Command specifies the action that you want to perform, such as get describe logs or executive.

Some commands show you information, while others allow you to change. The cluster's configuration type defines the kubernetes object that the command acts upon. For example, you could specify pods, deployments, nodes or other objects, including the cluster itself. Type used in combination with command. Tells Q CTL What you want to do and the type of object you want to perform. That action upon name specifies the object defined and type. The name field isn't always needed, especially when you're using commands that does list or show you information. For example, if you run the Command cube CTL, get pods without specifying a name. The command returns a list of all pods. To filter this list, you specify a pods name such as cube, CTL, get pod, my dash, test dash, app and cube. CTL then returns information only on the pod named my dash test dash app. Some commands support additional optional flags that you can include at the end of the command. Think of this as making a special request like formatting the output in a certain way. You could view the state of a pod by using the command cube CTL get pod my dash test dash app space Dash O equals mammal.

By the way, telling Q CTL to give you output in yama format is a really useful tool. You'll often want to capture the existing state of kubernetes object in a, a file, so that, for example, you can recreate it in a different cluster.

You can also use flags to display more information than you normally see. For instance, you can run the command cube CTL get pods space dash O equals wide to display the list of pods in wide format, which means you get to see additional columns of data on each of the pods in the list. One noteworthy piece of extra information you get in wide format is which note each pot is running on. You can do many things with the Cube CTL Command from creating kubernetes objects, interviewing them, deleting them and viewing or exporting configuration files. Remember to configure cube CTL first or to use the dash, dash cube config or dash dash context parameters so that the commands you type are performed on the cluster that you intended.

## Deployments

Deployments describe a desired state of pods. For example, a desired state could be that you want to make sure that you have five engine X pods running at all times. It's declarative stance means that Kubernetes will continuously make sure this configuration is running across your cluster. Kubernetes also supports various update mechanisms for deployments, which I'll tell you about later in this module. Deployments declare the state of pods. Every time you update the specification of the pods, for example, updating them to a newer container image, a new ReplicaSet is created that matches the altered version of the deployment. This is how deployments roll out updated pods in a controlled manner. Old pods are removed from the old ReplicaSet and replaced with newer pods in a new ReplicaSet. If the updated pods are not stable, the administrator can roll back the pods to a previous deployment revision. You can scale pods manually by modifying the deployment configuration. You can also configure the deployment to manage the workload automatically. Deployments are designed for stateless applications. Stateless applications don't store data or application state to a cluster or to persistent storage. A typical example of a stateless application is a web front-end. Some back end owns the problem of making sure that data gets stored durably, and you'll use Kubernetes objects other than deployments to manage these back ends. The desired state is described in a deployment YAML file containing the characteristics of the pods coupled with head operationally run these pods, and their life-cycle events. After you submit this file to the Kubernetes control plane, it creates a deployment controller which is responsible for converting the desired state into reality, and keeping that desired state over time. Remember what a controller is. It's a loop process created by Kubernetes, that takes care of the routine tasks to ensure the desired state of an object or a set of objects running on the cluster matches the observed state. During this process, a ReplicaSet is created. A ReplicaSet is a controller that ensures that a certain number of pod replicas are running at any given time. The deployment is a high level controller for a pod that declares it's state. The deployment configures a ReplicaSet controller to instantiate and maintain a specific version of the pods specified in the deployment. Here's a simple example of a deployment object file in YAML format. The deployment named my-app is created with three replicated pods. In the spec doc template section, a pod template defines the metadata and specification for each of the pods in this ReplicaSet. In the pod specification, an image is pulled from the Google Container Registry, and port 8080 is exposed to send and accept traffic for the container. Any deployment has three different life-cycle states. The deployments progressing state indicates that a task has been performed. What tasks? Creating a new ReplicaSet, or scaling up, or scaling down a ReplicaSet. The deployments complete state indicates that all new replicas have been updated to the latest version, and are available and no old replicas are running. Finally, the failed state occurs when the creation of a new ReplicaSet could not be completed. Why might this happen? Maybe Kubernetes couldn't pull the images for the new pods, or maybe there wasn't enough for some resource quota to complete the operation, or maybe the user who launched the operation lacks permissions. When you apply many small fixes across many roll outs, that translates to a large number of fusions and to management complexity. You have to remember which small fix was applied to which role, which can make it challenging to figure out which revision to roll back when issues arise. Remember earlier in the specialization, we recommended that you keep your YAML files in a source code repository. That can help manage some of this complexity.

### Ways to Create Deployments

You can create a deployment in three different ways. First, you create a deployment declaratively using a manifest file, such as a YAML file you've just seen, and a kubectl apply command. The second method creates a deployment imperatively using a kubectl run command that specifies the parameters in line. Here, the image and tag specifies which image, and image and version to run in the container. This deployment will launch three replicas and expose Pod 8080. Labels are defined using key and value dash dash generator specifies the API version to be used, and dash dash save dash config saves the configuration for future use. Your third option is to use the GKE workloads menu in the GCP console. Here you can specify the container, image and version or even select it directly from the container registry. You can specify environment variables and initialization commands. You can also add an application name and name space along with labels. You can use the View YAML button on the last page of the Deployment Wizard to view that deployments specification in YAML format. The ReplicaSet created by the deployment ensures set the desired number of pods are running and always available at any given time. If a pod fails or is evicted, the ReplicaSet automatically launches a new pod. You can use the kubectl get and describe commands to inspect the state and details of the deployment. As shown here, you can get the desired current up to date and available status of all the replicas within a deployment along with their ages using the kubectl get deployment command. Desired shows the desired number of replicas in the deployment specification. Current is the number of replicas currently running. Up-to-date, shows the number of replicas that are fully up-to-date as per the current deployment specification. Available, displays the number of replicas available to the users. You can also output the deployment configuration in a YAML format. This is a useful trick. Maybe you originally created a deployment with kubectl run. Then you decided you'd like to make it permanent managed part of your infrastructure. Edit that YAML file to remove the unique details of the deployment you created it from. Then you can add it to your repository of YAML files for future deployments. For more detailed information about the deployment, use the kubectl describe command. You'll learn more about this command in the lab. Another way to inspect a deployment is to use a GCP console, here you can see detailed information about the deployment, revision history, the pods events, and also view the live configuration in YAML format.

### Serices and Scaling

You now understand that deployment will maintain the desired number of replicas for an application. However, at some point, you'll probably need to scale the deployment. Maybe you need more web front end instances, for example. You can scale the deployment manually using a kubectl command, or the GCP console, by defining the number of replicas. Also manually changing the manifest will scale the deployment. You can also autoscale the deployment by specifying the minimum and maximum number of desired parts along with the CPU utilization threshold. Again, you can perform autoscaling by using the kubectl autoscale command or from the GCP console directly. This leads to the creation of a Kubernetes object called Horizontal Pod Autoscaler. This object performs the actual scaling to match the target CPU utilization. Keep in mind that we're not scaling the cluster as a whole, just a particular deployment within that cluster. Later in this module, you'll learn how to scale clusters.

### Updating Deployments

When you make a change to a Deployments pod specification, such as changing the image version an automatic update roll out will happen. Again, note that these automatic updates are only applicable to the changes in pod specifications. You can update a Deployment in different ways. One way is to use a kubectl apply command with an updated deployment specification file. This method allows you to update other specifications of a Deployment, such as the number of replicas outside the pod template. Another way is to use a kubectl set command. This allows you to change the pod template specification for the Deployment such as the image, resources or selector values. Another way is to use the kubectl edit command. This opens a specification file using the Vim editor that allows you to make changes directly. Once you exit and save the file, kubectl automatically applies the updated file. The last option for you to update a Deployment is through the GCP console. You can edit the Deployment manifest from the GCP console and perform a rolling update along with this additional options. Rolling updates are discussed next. Let's look at how deployment is updated. When a deployment is updated, it launches a new replica set and creates a new set of pods in a controlled fashion. First new pods are launched in a new replica set. Next, all pods are deleted from the old ReplicaSet. This is an example of a rolling update strategy, also known as a ramped strategy. It's advantage is that updates are slowly released, which ensures the availability of the application. However, this process can take time and there is no control over how the traffic is directed to the old and new pods.

### Blue-Green Deployments

We haven't yet discussed how to locate and connect to the applications running in these pods, especially as new pods are created or updated by your deployments. Well, you can connect to individual pods directly, pods themselves are transient. The kubernetes service is a static IP address that represents a service or a function in your infrastructure. It's a network abstraction for a set of pods that deliver that service in, it hides the ephemeral nature of the individual pods. Services will be covered in detail in the networking module. A blue green deployment strategy is useful when you want to deploy a new version of an application and also ensure that application services remain available while the deployment is updated. With a blue green update strategy, a completely new deployment is created with a newer version of the application. In this case, it's my-app-v2. When the pods in the new deployment are ready, the traffic can be switched from the old blue version to the new green version. But how can you do this? This is where a kubernetes service is used. Services allow you to manage the network traffic flows to a selection of pods, this set of pods is selected using a label selector. Let's take a closer look at how this is implemented. Here, in the service definition, pods are selected based on the label selector, where pods in this example belonged to my-app inter version v1. When a new deployment labeled v2 in this phase is created and is ready, the version label and the service is changed to the newer version, labeled v2 in this example. Now, the traffic will be directed to the newest set of pods, the green deployment with the v2 version label, instead of the old blue deployment pods that have the v1 version label. The blue deployment with the older version can then be deleted.

The advantage of this update strategy is that rollouts can be instantaneous and the newer versions can be tested internally before releasing them to the entire user base. For example, by using a separate service definition for test user access. The disadvantage is that the research usage is doubled during the deployment process.

### Canary Deployments

Hi, I'm Eoin Carroll, and I'll be teaching a couple of lectures in this course. In this video, we're going to be covering canary deployments. The Canary method is another update strategy based on the blue-green method, but traffic is gradually shifted to the new version. The main advantages of using canary deployments are that you can minimize the excess resource usage during the update. Because the rotor is gradual, issues can be identified before they affect all instances of the application. In this example, 100 percent of the application traffic is directed initially to my-app-V1. When the canary deployment starts, a subset of the traffic 10 percent in this case, is redirected to the new version, my-app-V2. When stability of the new version is confirmed, 100 percent of the traffic can be routed as a new version. But how is this done? In the blue-green update strategy previously covered both app and version labels were selected by the service. Traffic would only be sent to the pods that are running the version defined in the service. In a canary update strategy, the service selector is based on the application label and does not specify the version. The selector in this example covers all pods with the app colon, my-app label. This means that with this canary update strategy version of the service, traffic is sent to all pods regardless of the version label. This setting, allows a service to select and direct traffic to parts from both deployments. Initially, the new version of the deployment will start with 0 replicas running an overtime as a new version is scaled-up, the old version of deployment can be scaled down and eventually deleted. With the canary updates strategy a subset of users will be directed to the new version. This allows you to monitor for errors and performance issues as these users use the new version. You can quickly rollback, minimizing the impact to your overall user base if any issues arise. However, the complete rollout of a deployment using canary strategy can be a slow process and may require tools such as STO to accurately shift the traffic. There are other deployment strategies such as AB testing and shadow testing, but these strategies are outside the scope of this course. A service configuration does not normally ensure that all requests from a single client will always connect to the same pod. Each request is treated separately and can connect to any pod deployment, this potentially can cause issues if there are significant changes in the functionality between pods, as may happen with a canary deployment. To prevent this, you can set the session affinity field to client IP and the specification of the service. If you need a client's first request to determine which part will be used for all subsequent connections. That's rollout. Next, let's discuss how to roll back updates, especially in a rolling update and recreate strategies. You roll back using the kubectl rollout undo command. A simple rollout undo command will revert the deployment to its previous revision. You can roll back to a specific version by specifying the revision number. If you're not sure the changes, you can inspect the rollout history using kubectl rollout history command. The Cloud Console does not have a direct rollback feature. However, you can start Cloud Shell from your console and use these commands. The Cloud Console also shows you the revision list with summaries and creation dates. By default, the details of the ten previous replica sets are retained so that you can roll back to them. You can change it's defaults by specifying a revision history limits under the deployment specification.

### Managing Deployments

When you edit a deployment your action normally triggers an automatic rollout, but if you have an environment where small fixes are released frequently you'll have a large number of rollouts. In a situation like that, you'll find it more difficult to link issues with specific rollouts, to help you can temporarily pause these rollouts by using the kubectl rollout pause command. The initial state of the deployment prior to pausing will continue its function, but new updates suited deployment will not have any effect while the rollout is passed, the changes will only be implemented once a rollout is resumed. When you resume the rollout, all these new changes will be rolled out with a single revision. You can also monitor the rollout status by using the kubectl rollout status command. What if you're done with a deployment? You can delete it easily using the kubectl delete command, and you can also delete it from the GCP Console. Either way, kubenetes will delete all resources managed by the deployment, especially running pods.

## Pod Networking

The Kubernetes networking model relies heavily on IP addresses. Services, pods, Containers and nodes communicate using IP addresses and ports. Kubernetes provides different types of load balancing to direct traffic to the correct pod. Let's start by reviewing basic pod networking. Remember, a pod is a group of containers with shared storage and networking. This is based on the IP per pod model of Kubernetes. With this model, each pod is assigned a single IP address, and the containers within a pod, share the same network namespace, including that IP address. For example, you might have a legacy application that uses NGINX as a reverse proxy for client access. The NGINX container runs on TCP port 80 and the legacy application runs on TCP port 8,000. Because both containers share the same networking namespace, the two containers appear as though they are installed on the same machine. The NGINX container will contact the legacy application by establishing a connection to local host on TCP port 8,000. This works well for a single-part, but your workload doesn't run in single pod. Your workload is composed of many different applications that need to talk to each other. So how do pods talk to other pods? Each pod has a unique IP address, just like a host on the network. On a node, the pods are connected to each other through the nodes root network namespace, which ensures that pods can find and reach each other on that VM. This allows the two pods to communicate on the same node. The root network namespace is connected to the nodes primary NIC. Using the nodes VM NIC, the root network namespace is able to forward traffic out of that node. This means that the IP addresses on the pods must be routable on the network that the node is connected to. Where does the node get the IP address for the pods? In GKE, the nodes will get the pod IP addresses from address ranges assigned to your virtual private cloud or VPC. VPCs are logically isolated networks that provide connectivity for resources you deploy within GCP, such as Kubernetes clusters, Compute Engine instances, and app engine flex instances. A VPC can be composed of many different IP subnets in regions all around the world. When you deploy a GKE, you can select a VPC along with a region or zone. By default, a VPC has an IP subnet pre-allocated for each GCP region in the world. The IP addresses in the subnet are then allocated to the Compute instances that you deploy in that region. As you learned earlier in the specialization, GKE cluster nodes are compute instances that GKE customizes and manages for you. These machines are assigned IP addresses from the VPC subnet that they reside in. On GCP, NIC IPs allow you to configure additional secondary IP addresses or IP ranges on your Compute Engine virtual machine instances. VPC native GKE clusters automatically create an alias IP range to reserve approximately 4,000 IP addresses for cluster-wide services that you may create later. This mitigates the problem of unexpectedly running out of service IP addresses, which as you'll learn later, your applications use to talk to one another. VPC native GKE cluster also creates a separate alias IP range for your pods. Remember, each pod must have a unique address, so this address space will be large. By default, the address range uses a slash 14 block, which contains over 250,000 IP addresses, and that's a lot of pods. In reality, Google doesn't expect you to run 250,000 pods in a single cluster. Instead, that massive IP address range allows GKE to divide the IP space amongst the nodes. Using this large pod IP range, GKE allocates a much smaller slash 24 block to each node, which contains about 250 IP addresses. This allows for 1,000 nodes with overrunning 100 pods each by default. The number of nodes you expect to use, and the maximum number of pods per node are configurable, so you don't have to reserve a whole slash 14 for this.

## Volumes

Let's start by introducing volumes. In this video, you will learn about the types of storage abstractions that kubernetes provides, such as volumes and persistent volumes. You will learn about how these differ and how they can be used to store and share information between pods. Remember that kubernetes uses objects to represent the resources it manages. This rule applies to the storage as well as to the pods. All these objects functions as useful abstractions, which means that you can manage the resources they represent without laborious attention to implementation details. Kubernetes provide storage abstraction as volumes and persistent volumes. You learn about them in this lesson. Volumes are the method by which you attach storage to a pod.

Some volumes are ephemeral, which means they last only as long as the pod to which they are attached. You will see examples of these types in this lesson, such as config, map and empty dirt.

And some volumes are persistent, which means that they can outlive a pod regardless of type volumes are attached to pods, not containers. If a pod isn't mapped to a note anymore, the volume isn't either.

Other volume types can be used to provide storage that is persistent. This can be used for data that must outlive the life cycle of an individual pod. In Kubernetes cluster you will frequently find these volume types back in by NFS volumes or Windows shares or persistent disk from the underlying cloud provider. These types of volumes embody block storage or use network file systems on GKE These volumes are typically backed by compute engine, persistent disks. They provide durable storage beyond the existence of a pod. A failing node or pod shouldn't affect these volumes. If that happens, the volumes are simply unmounted from the feeling pod.

Some of these volumes might already exist before the creation of the pod and can be claimed and mounted.

### Volume Types

At its core, a volume is just a directory that is accessible to the containers in a pod. How that directory is created, the medium that backs it, and its contents are determined by the particular volume they used. In this case, we will create a pod with an NFS volume. The NFS server that backs this could be anywhere. In GCP, the lowest overhead way to serve NFS volumes is the cloud file storage managed service. In this example, we will create a pod using the kubectl create command using a pod manifest that includes an NFS volume type. Here, we're creating a pod with an NFS volume. A volume section is added under the pod's spec. The volume is named, and fields are configured with the access details for an existing NFS share. This creates an NFS volume when the pod is created. In order to mark the volume to a container, the volume name and mountPath must be specified under the volumeMounts field for the container. The volume is created during the pod creation. After the volume is created, it's available to any container in the pod before the containers are brought online. After the volume is attached to a container, the data in the volume is mounted into a container's file system. In this example, the files stored on the NFS share are made available on /mnt/vol directory inside the container. With the data mounted, the containers in the pod are brought online, and the rest of the pod initialization happens as before. In this example, you have an nginx pod with an NFS volume attached. The volume is attached at the mount volume directory when the pod is run. And then, the nginx container can see that /mnt/vol directory, and get data from it. For example, maybe the content that nginx serves is located in that share. Unlike the content of empty data volumes, which are erased when the pod is deleted, the data saved on NFS volumes will outlive the pod. When the pod is deleted, the NFS volume will be removed. However, the data isn't erased. It's just unmounted, and can be remounted on new pods, if needed.

Kubernetes PersistentVolume objects abstract storage provisioning from storage consumption. Remember, Kubernetes enables microservices architecture, where the application is decoupled into components that can be scaled easily. Persistent storage makes it possible to deal with failures and allow for dynamic rescheduling components without loss of data. However, should application developers be responsible for creating and maintaining separate volumes for their application components? Also, how can developers test applications before deploying into production without modifying the pod manifest for their application?

Whenever you have to reconfigure things to go from test to production, there's a risk of error. Kubernetes PersistentVolume obstruction resolves both of these issues. Using PersistentVolumes, a cluster administrator can provision a variety of volume types. The cluster administrator can simply provision storage and not worry about its consumption. An application developers can easily claim and use provision storage using PersistentVolumeClaims without creating or maintaining storage volumes directly. Notice the separation of roles. It's the job of the administrators to make PersistentVolumes available. And it's the job of the developers to use those volumes in applications. Due to job roles, can work independently of each other. When application developers use PersistentVolumeClaims, they don't need to worry about where the application is running. Provision storage capacity can be claimed by the application regardless of whether it's running on a local site or in Google cloud or on any other cloud provider.

Let's look at what's required for an application owner to consume compute engine persistent disk storage. First using part level volumes. Then using cluster level, PersistentVolumes and PersistentVolumeClaims in pod manifests. You'll see that the second way is much more manageable. In order to use PersistentVolumes, the operations team that owns the specific cloud implementation defines the storage classes and manages the actual implementation of the PersistentVolumes. The developers and application owners use PersistentVolumeClaims to request the quantity storage and the storage class, which determines the type of storage. This allows the operations team to manage the cloud services that they want to use. And allows the application owners to focus on what their application requires rather than on the specific implementation detail. Google clouds Compute Engine service uses persistent disks for virtual machine disks. And Kubernetes engine uses the same technology for PersistentVolumes.

Persistent disks are network based block storage that can provide durable storage. First, 100 gigabyte Compute Engine persistent disk is created using a gcloud command.

Before this persistent disk can be used by any pod, someone or some process must create it. And that person or process must have Compute Engine administration rights. As we've seen earlier with the nfs volume example, this Compute Engine persistent disk can be attached to a part by specifying a gce persistent disk volume type inside the pod manifest. In this pod manifest, the Compute Engine persistent disk volume is described in the gcePersistentDisk field as pd name, demo disc and the file system type as ext4.

The pd name must correspond to a Compute Engine persistent disk that has already been created.

This is the old way of attaching a persistent disk. And is no longer the best way to do it. I'm showing it to you here because you may see it in existing applications.

When the pod is created, kubelet uses Compute Engine API to attach the persistent disk to the node on which the pod is running.

The volume is automatically formatted and mounted to the container. If this pot is moved to another node, Kubernetes automatically detaches the persistent disk from the existing node and reattaches it to the new node. You can confirm that a volume has been mounted successfully by using the cube ctl described pod command. Don't forget that a persistent disk must be created before it can be used. The persistent disk can have pre populated data that can be shared by containers within a pod. That's very convenient. However, the application owner must include specific details of the persistent disk inside the pod manifesto further application. And must confirm that the persistent disk already exists. That's not convenient. Hard coding volume configuration information into parts specifications in this way means that you will have difficulty porting data from one cloud to another. And gce volumes are usually configured to use Compute Engine persistent disks. In your own premises Kubernetes cluster, they might be VMWare, vSphere, volume files, for example. Or even physical hard disks. Whenever you need to reconfigure an application to move it from one environment to another, there is a risk of error. To address this problem, Kubernetes provides an abstraction called PersistentVolumes. This obstruction that's pods claim volume of a certain size or of a certain name from a pool of storage without forcing you to define storage type details inside the pod specification,

### The PersistentVolume Abstraction

Let's take a closer look at how persistent volumes make the use of network storage more manageable. The persistent volume abstraction has two components, persistent volume and persistent volume claim. Persistent volumes are durable and persistent storage resources manage at the cluster level. Although these cluster resources are independent of the pod's life-cycle, a pod can use these resources during its life-cycle. However, if a pod is deleted, a persistent volume and it's data continue to exist. These volumes are managed by Kubernetes and can be manually or dynamically provisioned. GKE can use, compute, engine persistent disks as persistent volumes. Persistent volume claims are requests and claims made by pods to use persistent volumes. Within a persistent volume claim object, you define a volume's size, access mode, and storage class. What's a storage class? It's a set of storage characteristics that you have given a name to. You'll learn more about storage clusters later in this module. A pod uses this persistent volume claim to request a persistent volume. If a persistent volume matches all the requirements defined in a persistent volume claim, the persistent volume claim is bound to that persistent volume. Now, a pod can consume storage from this persistent volume. What's the critical difference between using pod level volumes and cluster level persistent volumes for storage? Persistent volumes provide a level of abstraction that lets you decouple storage administration from application configuration. The storage in a persistent volume must be bound with a Persistent Volume Claim in order to be accessed by a Pod. Here's how you create a Persistent Volume manifest for the same storage. Let's take a closer look at how this is used to make managing storage configurations for Pods easier. First, you specify the volume capacity, then the storage Class-Name. Storage class is a resource used to implement Persistent Volumes. Note that the PVC uses the storage Class-name. Then you define the PVC in a Pod, and it must match the PV storage Class-Name for the claim to succeed. GKE has a default storage Class-Name standard to use the Compute Engine standard persistent disk type, as shown here on the right. In this example, the PV definition on the left matches the GKE default storage class. In GKE clusters, a PVC with no defined storage class will use this default storage class and provide storage using a standard persistent disk. If you want to use an SSD persistent disk, you can create a new storage class, such as this example named SSD. A PVC that uses this new storage class named SSD will only use a PV that also has a storage class named SSD in this instance. And SSD persistent disk will be used. Once you create the new Storage class with the kubectl apply command, you can view it in the GCP Console. By the way, don't confuse Kubernetes storage classes with storage classes that Google Cloud storage makes available. Although the features have the same name, they are unrelated because they come from different services and govern different features. Google Cloud storage is object store for the web, while Kubernetes storage classes are choices for how Persistent Volumes are bagged.